\documentclass[11pt,]{article}
\usepackage[left=1in,top=1in,right=1in,bottom=1in]{geometry}
\newcommand*{\authorfont}{\fontfamily{phv}\selectfont}
\usepackage[]{mathpazo}


  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}




\usepackage{abstract}
\renewcommand{\abstractname}{}    % clear the title
\renewcommand{\absnamepos}{empty} % originally center

\renewenvironment{abstract}
 {{%
    \setlength{\leftmargin}{0mm}
    \setlength{\rightmargin}{\leftmargin}%
  }%
  \relax}
 {\endlist}

\makeatletter
\def\@maketitle{%
  \newpage
%  \null
%  \vskip 2em%
%  \begin{center}%
  \let \footnote \thanks
    {\fontsize{18}{20}\selectfont\raggedright  \setlength{\parindent}{0pt} \@title \par}%
}
%\fi
\makeatother




\setcounter{secnumdepth}{0}

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}

\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}


\title{Predicting Admission Probability in U.S Grad school using Linear Models
and Machine Learning Algorithms \thanks{S.V Miller for providing the Pandoc template: github.com/svmiller}  }



\author{\Large Arumugam Thiagarajan\vspace{0.05in} \newline\normalsize\emph{Professional Certificate in Data Science, Harvard University}  }


\date{}

\usepackage{titlesec}

\titleformat*{\section}{\normalsize\bfseries}
\titleformat*{\subsection}{\normalsize\itshape}
\titleformat*{\subsubsection}{\normalsize\itshape}
\titleformat*{\paragraph}{\normalsize\itshape}
\titleformat*{\subparagraph}{\normalsize\itshape}





\newtheorem{hypothesis}{Hypothesis}
\usepackage{setspace}


% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

% move the hyperref stuff down here, after header-includes, to allow for - \usepackage{hyperref}

\makeatletter
\@ifpackageloaded{hyperref}{}{%
\ifxetex
  \PassOptionsToPackage{hyphens}{url}\usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \PassOptionsToPackage{hyphens}{url}\usepackage[draft,unicode=true]{hyperref}
\fi
}

\@ifpackageloaded{color}{
    \PassOptionsToPackage{usenames,dvipsnames}{color}
}{%
    \usepackage[usenames,dvipsnames]{color}
}
\makeatother
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={Arumugam Thiagarajan (Professional Certificate in Data Science, Harvard University)},
             pdfkeywords = {house rent, machine learning, linear models},  
            pdftitle={Predicting Admission Probability in U.S Grad school using Linear Models
and Machine Learning Algorithms},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls

% Add an option for endnotes. -----


% add tightlist ----------
\providecommand{\tightlist}{%
\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% add some other packages ----------

% \usepackage{multicol}
% This should regulate where figures float
% See: https://tex.stackexchange.com/questions/2275/keeping-tables-figures-close-to-where-they-are-mentioned
\usepackage[section]{placeins}


\begin{document}
	
% \pagenumbering{arabic}% resets `page` counter to 1 
%
% \maketitle

{% \usefont{T1}{pnc}{m}{n}
\setlength{\parindent}{0pt}
\thispagestyle{plain}
{\fontsize{18}{20}\selectfont\raggedright 
\maketitle  % title \par  

}

{
   \vskip 13.5pt\relax \normalsize\fontsize{11}{12} 
\textbf{\authorfont Arumugam Thiagarajan} \hskip 15pt \emph{\small Professional Certificate in Data Science, Harvard University}   

}

}








\begin{abstract}

    \hbox{\vrule height .2pt width 39.14pc}

    \vskip 8.5pt % \small 

\noindent The project builds and compares linear and suite of machine learning
algorithms that predict the admission probabilty of applicants to United
States Graduate Schools. The applicant characterisitics and academic
standings such as, TOEFL scores, GRE scores, Cumulative Grade Point
Average (CGPA), Letter of Recommendation (LOR) and Statement of Purpose
are some of the features that are used to predict their probability of
university admission. A regression based approach was used and the
dataset was explored for trends. Dataset was cleansed with relevant
features and models models. were built using linear regression and a
machine learning algorithms. Training and validation datasets were
established at a 50:50 proportion at random. Root Mean Square Error and
R2 values were used as measures of performance and the results revealed
that GLMnet achieved a higher level of accuracy compared any other
models. The RMSE values were at 0.065 with an r2 value of 0.887, better
than the ensemble or linear regression


\vskip 8.5pt \noindent \emph{Keywords}: house rent, machine learning, linear models \par

    \hbox{\vrule height .2pt width 39.14pc}



\end{abstract}


\vskip -8.5pt

{
\hypersetup{linkcolor=black}
\setcounter{tocdepth}{2}
\tableofcontents
}

 % removetitleabstract

\noindent  

\hypertarget{objective}{%
\section{Objective}\label{objective}}

Predict the admission rates of United States Graduate Schools using the
academic scores of the applicants and university rankings. Both general
linear models and machine learning algorithms will be used and their
performances will be compared. Root Mean Square and R2 will be used as
the measure of performance for the models.

\hypertarget{materials-and-methods}{%
\section{Materials and Methods}\label{materials-and-methods}}

\hypertarget{input-data}{%
\subsection{Input Data}\label{input-data}}

The original data is available for public at the following url:
www.kaggle.com/mohansacharya/ graduate-admissions Since a direct
download from kaggle requires an authentication, the whole dataset is
uploaded to a github account. The data and codes are downloaded from the
following github repository.
\url{https://github.com/HexyCodes/Admission.git}. This dataset contains,
400 rows of data and 8 features.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{url=}\StringTok{"https://raw.githubusercontent.com/HexyCodes/Admission/master/US_grad_admission.csv"}
\NormalTok{adm=}\KeywordTok{read_csv}\NormalTok{(url)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Parsed with column specification:
## cols(
##   `Serial No.` = col_double(),
##   `GRE Score` = col_double(),
##   `TOEFL Score` = col_double(),
##   `University Rating` = col_double(),
##   SOP = col_double(),
##   LOR = col_double(),
##   CGPA = col_double(),
##   Research = col_double(),
##   `Chance of Admit` = col_double()
## )
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(adm) }\CommentTok{# find the dimensions of the data.frame}
\end{Highlighting}
\end{Shaded}

{[}1{]} 400 9

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{colnames}\NormalTok{(adm)=}\KeywordTok{c}\NormalTok{(}\StringTok{"Serial.No"}\NormalTok{, }\StringTok{"GRE.Score"}\NormalTok{, }\StringTok{"TOEFL.Score"}\NormalTok{, }\StringTok{"University.Rating"}\NormalTok{, }
                \StringTok{"SOP"}\NormalTok{, }\StringTok{"LOR"}\NormalTok{, }\StringTok{"CGPA"}\NormalTok{, }\StringTok{"Research"}\NormalTok{, }\StringTok{"Chance.of.Admit"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{exploratory-data-analysis}{%
\subsection{Exploratory Data Analysis}\label{exploratory-data-analysis}}

In this exploration, the data is analyzed with their summarized
characteristics and examined through visualization charts. This step
allows to find the patterns, trends and any anamolies that may exist in
the data. The serial number column has been removed. This was an obvious
choice as this would add unncessary noise to the modeling process.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(adm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "spec_tbl_df" "tbl_df"      "tbl"         "data.frame"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(adm,}\DecValTok{5}\NormalTok{) }\CommentTok{# look at the data type of columns}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 5 x 9
##   Serial.No GRE.Score TOEFL.Score University.Rati~   SOP   LOR  CGPA Research
##       <dbl>     <dbl>       <dbl>            <dbl> <dbl> <dbl> <dbl>    <dbl>
## 1         1       337         118                4   4.5   4.5  9.65        1
## 2         2       324         107                4   4     4.5  8.87        1
## 3         3       316         104                3   3     3.5  8           1
## 4         4       322         110                3   3.5   2.5  8.67        1
## 5         5       314         103                2   2     3    8.21        0
## # ... with 1 more variable: Chance.of.Admit <dbl>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\KeywordTok{anyNA}\NormalTok{(adm))}\CommentTok{# check for missing values}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(adm) }\CommentTok{# quantile distribution of the predicted values}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Serial.No       GRE.Score      TOEFL.Score    University.Rating
##  Min.   :  1.0   Min.   :290.0   Min.   : 92.0   Min.   :1.000    
##  1st Qu.:100.8   1st Qu.:308.0   1st Qu.:103.0   1st Qu.:2.000    
##  Median :200.5   Median :317.0   Median :107.0   Median :3.000    
##  Mean   :200.5   Mean   :316.8   Mean   :107.4   Mean   :3.087    
##  3rd Qu.:300.2   3rd Qu.:325.0   3rd Qu.:112.0   3rd Qu.:4.000    
##  Max.   :400.0   Max.   :340.0   Max.   :120.0   Max.   :5.000    
##       SOP           LOR             CGPA          Research     
##  Min.   :1.0   Min.   :1.000   Min.   :6.800   Min.   :0.0000  
##  1st Qu.:2.5   1st Qu.:3.000   1st Qu.:8.170   1st Qu.:0.0000  
##  Median :3.5   Median :3.500   Median :8.610   Median :1.0000  
##  Mean   :3.4   Mean   :3.453   Mean   :8.599   Mean   :0.5475  
##  3rd Qu.:4.0   3rd Qu.:4.000   3rd Qu.:9.062   3rd Qu.:1.0000  
##  Max.   :5.0   Max.   :5.000   Max.   :9.920   Max.   :1.0000  
##  Chance.of.Admit 
##  Min.   :0.3400  
##  1st Qu.:0.6400  
##  Median :0.7300  
##  Mean   :0.7244  
##  3rd Qu.:0.8300  
##  Max.   :0.9700
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{adm}\OperatorTok{%>%}\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Chance.of.Admit))}\OperatorTok{+}\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{fill=}\StringTok{"blue"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_bw}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\includegraphics{USGradAdmission_files/figure-latex/exploratory_analysis-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{# distribution }
\CommentTok{#pattern of the predicted value}
\KeywordTok{qqnorm}\NormalTok{(adm}\OperatorTok{$}\NormalTok{Chance.of.Admit)}
\end{Highlighting}
\end{Shaded}

\includegraphics{USGradAdmission_files/figure-latex/exploratory_analysis-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{adm}\OperatorTok{%>%}\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{Serial.No)->adm }\CommentTok{# Remove Serial number from the daa. }
\CommentTok{# distribution all predictors in the data frame}
\KeywordTok{plot_density}\NormalTok{(adm, }
             \DataTypeTok{geom_density_args =} \KeywordTok{list}\NormalTok{(}\StringTok{"fill"}\NormalTok{=}\StringTok{"blue"}\NormalTok{, }
                                      \StringTok{"alpha"}\NormalTok{=}\FloatTok{0.6}\NormalTok{), }
             \DataTypeTok{ggtheme =} \KeywordTok{theme_bw}\NormalTok{()) }
\end{Highlighting}
\end{Shaded}

\includegraphics{USGradAdmission_files/figure-latex/exploratory_analysis-3.pdf}

\hypertarget{removing-extreme-values}{%
\subsection{Removing Extreme values}\label{removing-extreme-values}}

Histogram and boxplots of the `Chance of Admission' vector indicates a
normal distribution for all the features. It is a recommended practice
to examine the dataset for outliers. Therefore, a Inter quantile range
(IQR) methodology was used to identify the ``proposed outliers''. First,
the Q1 and Q3 quantile are identified, then the IRQ was calculated as
the difference between the Q3 and Q1. The range of values that exist
below the IQR\emph{1.5 or above IQR}1.5 were eliminated for this
project. From the results, only two rows were identified as potential
outliers. Considering the low occurrence of these values, the dataset is
being used as such without removal of outliers.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{any}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(adm)) }\CommentTok{# Checking for any missing values}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Function to check the outliers}
\NormalTok{IQR.outliers <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
  \ControlFlowTok{if}\NormalTok{(}\KeywordTok{any}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(x)))}
    \KeywordTok{stop}\NormalTok{(}\StringTok{"x is missing values"}\NormalTok{)}
  \ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.numeric}\NormalTok{(x))}
    \KeywordTok{stop}\NormalTok{(}\StringTok{"x is not numeric"}\NormalTok{)}
\NormalTok{  Q3<-}\KeywordTok{quantile}\NormalTok{(x,}\FloatTok{0.75}\NormalTok{)}
\NormalTok{  Q1<-}\KeywordTok{quantile}\NormalTok{(x,}\FloatTok{0.25}\NormalTok{)}
\NormalTok{  IQR<-(Q3}\OperatorTok{-}\NormalTok{Q1)}
\NormalTok{  left<-}\StringTok{ }\NormalTok{(Q1}\OperatorTok{-}\NormalTok{(}\FloatTok{1.5}\OperatorTok{*}\NormalTok{IQR))}
  \KeywordTok{print}\NormalTok{(left)}

\NormalTok{  right<-}\StringTok{ }\NormalTok{(Q3}\OperatorTok{+}\NormalTok{(}\FloatTok{1.5}\OperatorTok{*}\NormalTok{IQR))}
    \KeywordTok{print}\NormalTok{(right)}
  \KeywordTok{c}\NormalTok{(x[x }\OperatorTok{<}\NormalTok{left],x[x}\OperatorTok{>}\NormalTok{right])}
\NormalTok{\}}


\CommentTok{#list of outliers}
\NormalTok{outliers=}\KeywordTok{IQR.outliers}\NormalTok{(adm}\OperatorTok{$}\NormalTok{Chance.of.Admit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   25% 
## 0.355 
##   75% 
## 1.115
\end{verbatim}

\hypertarget{check-for-correlations}{%
\subsection{Check for correlations}\label{check-for-correlations}}

The dataset is examined for correlations among the different features.
There seems to a be strong correlation (\textgreater{}50\%) between all
of the features, such as GRE. Score, TOEFL. Score, University. Rating,
SOP, LOR, CGPA and Research. The boxplot on the important features
reveals a positive relationship. This is an interesting trend, because
many of these characteristics have confounding effects or colinearity
that exist with them. For instance, a person scoring high in GRE has a
high probability of scoring high in TOEFL and potentially writing a
worthy statement of purpose. Therefore, it is essential to examine
partial correlation coefficients of these features on the admisssion
chances.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Converting the data into matrix format for conduction correlation analysis}
\KeywordTok{data.matrix}\NormalTok{(adm)->adm_mat }
\CommentTok{# plotting the hrent matrix results}
\KeywordTok{plot_correlation}\NormalTok{(adm_mat) }
\end{Highlighting}
\end{Shaded}

\includegraphics{USGradAdmission_files/figure-latex/correlation_analysis-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plotting the correlation strength through size of squares.}
\KeywordTok{corrplot}\NormalTok{(}\KeywordTok{cor}\NormalTok{(adm_mat), }\DataTypeTok{method =} \StringTok{"square"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{USGradAdmission_files/figure-latex/correlation_analysis-2.pdf}

\hypertarget{visualizing-the-relationship}{%
\subsection{Visualizing the
relationship}\label{visualizing-the-relationship}}

This step further explores the relationship by visualizing the spread of
the features and presents the relationship between combination of
features in influencing the admission rates. I explore the impacts of
TOEFL. Score, GRE.Score and CGPA on Admission grouped by University
Rating. The trend appears to be linear and there is strong evidence that
these features are positively related to the admission rates, however
their magnitudes vary by Universities.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{adm}\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{TOEFL.Score,}\DataTypeTok{y=}\NormalTok{Chance.of.Admit, }\DataTypeTok{fill=}\KeywordTok{factor}\NormalTok{(University.Rating))) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{() }\OperatorTok{+}\KeywordTok{theme_bw}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Effects of University Rating and TOEFL scores on admission"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{USGradAdmission_files/figure-latex/boxplots_features-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{adm}\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{GRE.Score,}
             \DataTypeTok{y=}\NormalTok{Chance.of.Admit, }\DataTypeTok{fill=}\KeywordTok{factor}\NormalTok{(University.Rating))) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{() }\OperatorTok{+}\KeywordTok{theme_bw}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Effects of University Rating and GRE scores on admission"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{USGradAdmission_files/figure-latex/boxplots_features-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{adm}\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{CGPA,}\DataTypeTok{y=}\NormalTok{Chance.of.Admit, }\DataTypeTok{fill=}\KeywordTok{factor}\NormalTok{(University.Rating))) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{() }\OperatorTok{+}\KeywordTok{theme_bw}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Effects of University Rating and CGPA scores on admission"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{USGradAdmission_files/figure-latex/boxplots_features-3.pdf}

\hypertarget{partial-correlation-coefficient}{%
\subsection{Partial correlation
coefficient}\label{partial-correlation-coefficient}}

Beyond the correlation coefficients, the partial correlations reveal the
influence of individual attributes to the dependent variables of
interest. Furthermore, partial correlation ensures that the confounding
effects that exist in the variables are eliminated. This probability
values from the partial correlation coefficient, shows that the SOP and
University.Rating had no influence when partial correlation values were
considered (p\textgreater{}0.05). Based on these findings, SOP and
`University.Rating' features will be cleansed from our dataset before
the model development.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{partials=}\KeywordTok{pcor}\NormalTok{(adm_mat) }\CommentTok{# Conducting partial correlation analysis}
\KeywordTok{print}\NormalTok{(}\StringTok{"Partial Correlations for the Dependent Variable: Rent"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Partial Correlations for the Dependent Variable: Rent"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Estimates=}\KeywordTok{data.frame}\NormalTok{(partials}\OperatorTok{$}\NormalTok{estimate[,}\DecValTok{7}\OperatorTok{:}\DecValTok{8}\NormalTok{])}
\NormalTok{P.values=}\KeywordTok{data.frame}\NormalTok{(partials}\OperatorTok{$}\NormalTok{p.value[, }\DecValTok{7}\OperatorTok{:}\DecValTok{8}\NormalTok{])}
\CommentTok{# printing the results}
\KeywordTok{kable}\NormalTok{((Estimates), }\DataTypeTok{format =} \StringTok{"pandoc"}\NormalTok{, }\DataTypeTok{digits=}\DecValTok{2}\NormalTok{, }
      \DataTypeTok{caption=}\StringTok{"Partial correlations  of the input dataset features"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrr@{}}
\caption{Partial correlations of the input dataset
features}\tabularnewline
\toprule
& Research & Chance.of.Admit\tabularnewline
\midrule
\endfirsthead
\toprule
& Research & Chance.of.Admit\tabularnewline
\midrule
\endhead
GRE.Score & 0.26 & 0.15\tabularnewline
TOEFL.Score & -0.07 & 0.13\tabularnewline
University.Rating & 0.01 & 0.06\tabularnewline
SOP & 0.08 & -0.03\tabularnewline
LOR & -0.01 & 0.20\tabularnewline
CGPA & -0.05 & 0.44\tabularnewline
Research & 1.00 & 0.15\tabularnewline
Chance.of.Admit & 0.15 & 1.00\tabularnewline
\bottomrule
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{kable}\NormalTok{((P.values), }\DataTypeTok{format =} \StringTok{"pandoc"}\NormalTok{, }\DataTypeTok{digits=}\DecValTok{2}\NormalTok{, }
      \DataTypeTok{caption=}\StringTok{"Probability values  of the input dataset features"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrr@{}}
\caption{Probability values of the input dataset
features}\tabularnewline
\toprule
& Research & Chance.of.Admit\tabularnewline
\midrule
\endfirsthead
\toprule
& Research & Chance.of.Admit\tabularnewline
\midrule
\endhead
GRE.Score & 0.00 & 0.00\tabularnewline
TOEFL.Score & 0.17 & 0.01\tabularnewline
University.Rating & 0.78 & 0.23\tabularnewline
SOP & 0.10 & 0.55\tabularnewline
LOR & 0.87 & 0.00\tabularnewline
CGPA & 0.36 & 0.00\tabularnewline
Research & 0.00 & 0.00\tabularnewline
Chance.of.Admit & 0.00 & 0.00\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{model-development}{%
\section{Model Development}\label{model-development}}

\hypertarget{data-cleansing}{%
\subsection{Data Cleansing}\label{data-cleansing}}

Based on the partial correlation coefficient analysis, the SOP and the
University Rating features are removed from the dataset. Only this
dataset with reduced features will be further used for all of the
modeling efforts.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# data cleansed after removing SOP}
\NormalTok{adm}\OperatorTok{%>%}\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{SOP, }\OperatorTok{-}\NormalTok{University.Rating)->admfea }
\end{Highlighting}
\end{Shaded}

\hypertarget{splitting-data-into-training-and-validation-datasets.}{%
\subsection{Splitting data into training and validation
datasets.}\label{splitting-data-into-training-and-validation-datasets.}}

The data is split into two datasets. One for training and validation.
The training dataset will be used for model development and the
validation dataset will only be used for validation of the model as a
final step. Fifty percent of the cleansed data was chosen as the
validation dataset at random (318) and the rest (82) was saved as the
training dataset. This proportion was chosen based on the strength of
the features and their potential relationship with the admission rates.
The features selected were GRE.Score, TOEFL.Score, University.Rating,
LOR, CGPA and Research

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{sample.kind =} \StringTok{"Rounding"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in set.seed(1, sample.kind = "Rounding"): non-uniform 'Rounding' sampler
## used
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test_indices=}\KeywordTok{createDataPartition}\NormalTok{(admfea}\OperatorTok{$}\NormalTok{Chance.of.Admit, }
                                 \DataTypeTok{times=}\DecValTok{1}\NormalTok{, }
                                 \DataTypeTok{p=}\FloatTok{0.5}\NormalTok{, }\CommentTok{# portion of data split into test}
                                 \DataTypeTok{list=}\NormalTok{F)}
\NormalTok{admfea[}\OperatorTok{-}\NormalTok{test_indices,]->traindf }\CommentTok{# dataset reserved for training}
\NormalTok{admfea[test_indices,]->valdf }\CommentTok{# dataset held for validation}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: The `i` argument of ``[`()` can't be a matrix as of tibble 3.0.0.
## Convert to a vector.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.
\end{verbatim}

\hypertarget{model-performance-metrics}{%
\subsection{Model Performance Metrics}\label{model-performance-metrics}}

The performance of the models are evaluated through two metrics. First,
the Root mean square error (RMSE) and second with R2, coefficient of
determination, between the acutals and predicted values.

RMSE
\[ RMSE = \sqrt{\frac{1}{n}\Sigma_{i=1}^{n}{\Big(\frac{yhat -y}{\sigma_i}\Big)^2}} \]

where, yhat is the predicted value of y, y is the actual value, n is the
number of samples.

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{approach-and-model-development}{%
\subsection{Approach and Model
Development}\label{approach-and-model-development}}

From the datatype of the dependent variable and intial data exploration,
it is evident that this is a regression problem. Accordingly, regression
based modeling solutions will be explored for model development.
Initially, a general linear model will be built using all of the
features in the dataset. Following this a feature reduction step will be
performed for the linear models using a stepwise regression. A backward
and foward propagated stepwise regression will be performed and the
model that exhibits the lowest AIC score will be selected. The AIC
refers to the Akaike Information Criteria that defines the performance
of the model chosen through a penalization procedure.

\hypertarget{tuning-the-parameters}{%
\subsection{Tuning the parameters}\label{tuning-the-parameters}}

This section of the code was run for selecting the optimized parmeters
for the Rborist. The predFixed parameter (mtry in RandomForest) and the
minNode size model parameters are checked. Given the limitation of the
resources, this code is commented out.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# cl=makePSOCKcluster(detectCores())}
\CommentTok{# registerDoParallel(cl)}
\CommentTok{# #Create control function for training with 10 folds}
\CommentTok{# #and keep 3 folds for training. search method is grid.}
\CommentTok{# }
\CommentTok{# control <- trainControl(method='repeatedcv',}
\CommentTok{#                         number=10,}
\CommentTok{#                         repeats=3,}
\CommentTok{#                         search='grid')}
\CommentTok{# }
\CommentTok{# tunegrid <- expand.grid(predFixed = c(1:5), minNode=1:3)}
\CommentTok{# rf_gridsearch <- train(Chance.of.Admit ~ .,}
\CommentTok{#                        data = trainset,}
\CommentTok{#                        method = 'Rborist',}
\CommentTok{#                        metric = c("RMSE"),}
\CommentTok{#                       tuneGrid = tunegrid)}
\CommentTok{# print(rf_gridsearch)}
\CommentTok{# stopCluster(cl)}
\end{Highlighting}
\end{Shaded}

\hypertarget{linear-model}{%
\subsection{Linear Model}\label{linear-model}}

The general linear model considers Chance.of.Admit as the dependent
variable and all of the other features as the independent variables. The
linear model summary showed that the GRE and TOEFL scores were not
significantly correlated with the predicted variable. This seems
unreasonable and potentially confounding or multicolinearity effects are
masking their effects. Therefore, a stepwise regression procedure was
implemented to identify the most influential factors. The results
revealed that Chance.of.Admit \textasciitilde{} GRE.Score + TOEFL.Score
+ LOR + CGPA were chosen as the factors for the model. Accordingly, a
lm.model.step was constructed. Note that the R2 values were similar on
both of the models.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#General linear model}
\NormalTok{mod.lm=}\KeywordTok{lm}\NormalTok{(Chance.of.Admit}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{traindf) }
\KeywordTok{summary}\NormalTok{(mod.lm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = Chance.of.Admit ~ ., data = traindf)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.23633 -0.02649  0.00890  0.03769  0.15988 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -1.2737301  0.1744318  -7.302 7.24e-12 ***
## GRE.Score    0.0014463  0.0009021   1.603 0.110516    
## TOEFL.Score  0.0020728  0.0014942   1.387 0.166967    
## LOR          0.0240550  0.0070353   3.419 0.000766 ***
## CGPA         0.1431870  0.0169714   8.437 7.59e-15 ***
## Research     0.0104297  0.0115710   0.901 0.368516    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.06315 on 193 degrees of freedom
## Multiple R-squared:  0.8146, Adjusted R-squared:  0.8098 
## F-statistic: 169.6 on 5 and 193 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#predicting results from the model}
\NormalTok{pred.lm=}\KeywordTok{predict}\NormalTok{(mod.lm, }\DataTypeTok{newdata=}\NormalTok{valdf)}
\KeywordTok{RMSE}\NormalTok{(pred.lm, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.0655762
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Stepwise Regression procedure}
\KeywordTok{stepAIC}\NormalTok{(mod.lm, }\DataTypeTok{direction=}\StringTok{"both"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Start:  AIC=-1093.45
## Chance.of.Admit ~ GRE.Score + TOEFL.Score + LOR + CGPA + Research
## 
##               Df Sum of Sq     RSS     AIC
## - Research     1  0.003240 0.77298 -1094.6
## - TOEFL.Score  1  0.007675 0.77742 -1093.5
## <none>                     0.76974 -1093.5
## - GRE.Score    1  0.010251 0.77999 -1092.8
## - LOR          1  0.046627 0.81637 -1083.7
## - CGPA         1  0.283897 1.05364 -1033.0
## 
## Step:  AIC=-1094.61
## Chance.of.Admit ~ GRE.Score + TOEFL.Score + LOR + CGPA
## 
##               Df Sum of Sq     RSS     AIC
## - TOEFL.Score  1  0.007085 0.78007 -1094.8
## <none>                     0.77298 -1094.6
## + Research     1  0.003240 0.76974 -1093.5
## - GRE.Score    1  0.014482 0.78747 -1092.9
## - LOR          1  0.048971 0.82195 -1084.4
## - CGPA         1  0.295041 1.06802 -1032.3
## 
## Step:  AIC=-1094.79
## Chance.of.Admit ~ GRE.Score + LOR + CGPA
## 
##               Df Sum of Sq     RSS     AIC
## <none>                     0.78007 -1094.8
## + TOEFL.Score  1   0.00709 0.77298 -1094.6
## + Research     1   0.00265 0.77742 -1093.5
## - GRE.Score    1   0.03368 0.81375 -1088.4
## - LOR          1   0.05524 0.83531 -1083.2
## - CGPA         1   0.35233 1.13240 -1022.6
\end{verbatim}

\begin{verbatim}
## 
## Call:
## lm(formula = Chance.of.Admit ~ GRE.Score + LOR + CGPA, data = traindf)
## 
## Coefficients:
## (Intercept)    GRE.Score          LOR         CGPA  
##   -1.367540     0.002218     0.025851     0.151528
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Predicting results from stepwise regression}
\NormalTok{mod.lm.step=}\KeywordTok{lm}\NormalTok{(Chance.of.Admit}\OperatorTok{~}\NormalTok{GRE.Score}\OperatorTok{+}
\StringTok{                 }\NormalTok{TOEFL.Score}\OperatorTok{+}\NormalTok{LOR}\OperatorTok{+}\NormalTok{CGPA, }\DataTypeTok{data=}\NormalTok{traindf)}
\KeywordTok{summary}\NormalTok{(mod.lm.step)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = Chance.of.Admit ~ GRE.Score + TOEFL.Score + LOR + 
##     CGPA, data = traindf)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.243878 -0.026094  0.007705  0.037273  0.156825 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -1.3432107  0.1563998  -8.588 2.87e-15 ***
## GRE.Score    0.0016591  0.0008703   1.906 0.058068 .  
## TOEFL.Score  0.0019876  0.0014905   1.333 0.183936    
## LOR          0.0245705  0.0070086   3.506 0.000565 ***
## CGPA         0.1449726  0.0168472   8.605 2.58e-15 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.06312 on 194 degrees of freedom
## Multiple R-squared:  0.8139, Adjusted R-squared:   0.81 
## F-statistic:   212 on 4 and 194 DF,  p-value: < 2.2e-16
\end{verbatim}

\hypertarget{machine-learning-models}{%
\subsection{Machine Learning Models}\label{machine-learning-models}}

For machine learning, the following models were chosen: Rborist, which
is a fast implementation of the random forest, k-nearest neighbours,
neural net implmentation of glm (glmnet), gradient boost algorithm
(xgbLinear), bayesian regularized neural network (brnn) and ridge
regression. All of these models were run in the CareEnsemble package.
This packages allows simulataneous runs of various ML algorithms,
collect and integrate the results and above all, conducts an ensemble
evaluation of the model results based on their performance. In this
case, the RMSE will be used to evaluate the performance of the models by
defaults and the results are weighted according to the model RMSE
values.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{sample.kind =} \StringTok{"Rounding"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in set.seed(1, sample.kind = "Rounding"): non-uniform 'Rounding' sampler
## used
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cl=}\KeywordTok{makePSOCKcluster}\NormalTok{(}\KeywordTok{detectCores}\NormalTok{()}\OperatorTok{-}\DecValTok{1}\NormalTok{) }\CommentTok{#setting up clusters for parallel processing}
\KeywordTok{registerDoParallel}\NormalTok{(cl) }\CommentTok{# register the clusters for parellel processing}

\NormalTok{my.con=}\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method=}\StringTok{"cv"}\NormalTok{, }\CommentTok{# choose cross validation}
                    \DataTypeTok{number=}\DecValTok{3}\NormalTok{, }\CommentTok{# number of times the process is run}
                    \DataTypeTok{savePredictions =} \StringTok{"final"}\NormalTok{, }\DataTypeTok{allowParallel =}\NormalTok{ T)}
\NormalTok{models=}\KeywordTok{caretList}\NormalTok{(Chance.of.Admit}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{traindf,}
\DataTypeTok{trainControl=}\NormalTok{my.con, }
\DataTypeTok{methodList =} \KeywordTok{c}\NormalTok{(}\StringTok{"Rborist"}\NormalTok{,  }\CommentTok{# list of ML models chosen}
               \StringTok{"knn"}\NormalTok{, }
               \StringTok{"glmnet"}\NormalTok{,}
               \StringTok{"xgbLinear"}\NormalTok{,}
               \StringTok{"brnn"}\NormalTok{, }
               \StringTok{"ridge"}\NormalTok{), }
\DataTypeTok{continue_on_fail =}\NormalTok{ T) }\CommentTok{# making sure all models are running without error}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in trControlCheck(x = trControl, y = target): trControl$savePredictions
## not 'all' or 'final'. Setting to 'final' so we can ensemble the models.
\end{verbatim}

\begin{verbatim}
## Warning in trControlCheck(x = trControl, y = target): indexes not defined in
## trControl. Attempting to set them ourselves, so each model in the ensemble will
## have the same resampling indexes.
\end{verbatim}

\begin{verbatim}
## [22:17:00] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
## [22:17:00] WARNING: amalgamation/../src/learner.cc:480: 
## Parameters: { trainControl } might not be used.
## 
##   This may not be accurate due to some parameters are only used in language bindings but
##   passed down to XGBoost core.  Or some parameters are not used but slip through this
##   verification. Please open an issue if you find above cases.
## 
## 
## Number of parameters (weights and biases) to estimate: 7 
## Nguyen-Widrow method
## Scaling factor= 0.7 
## gamma= 6.7034     alpha= 1.6038   beta= 12.0324
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{models}\OperatorTok{$}\NormalTok{xgbLinear}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## eXtreme Gradient Boosting 
## 
## 199 samples
##   5 predictor
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 199, 199, 199, 199, 199, 199, ... 
## Resampling results across tuning parameters:
## 
##   lambda  alpha  nrounds  RMSE        Rsquared   MAE       
##   0e+00   0e+00   50      0.08063889  0.7109212  0.05853937
##   0e+00   0e+00  100      0.08063882  0.7109211  0.05853927
##   0e+00   0e+00  150      0.08063874  0.7109211  0.05853918
##   0e+00   1e-04   50      0.08058688  0.7117638  0.05854805
##   0e+00   1e-04  100      0.08058681  0.7117637  0.05854795
##   0e+00   1e-04  150      0.08058673  0.7117637  0.05854785
##   0e+00   1e-01   50      0.07457953  0.7465389  0.05386809
##   0e+00   1e-01  100      0.07457953  0.7465389  0.05386809
##   0e+00   1e-01  150      0.07457953  0.7465389  0.05386809
##   1e-04   0e+00   50      0.08059165  0.7113760  0.05860778
##   1e-04   0e+00  100      0.08059158  0.7113759  0.05860768
##   1e-04   0e+00  150      0.08059151  0.7113758  0.05860759
##   1e-04   1e-04   50      0.08057632  0.7115631  0.05863154
##   1e-04   1e-04  100      0.08057625  0.7115630  0.05863144
##   1e-04   1e-04  150      0.08057617  0.7115630  0.05863134
##   1e-04   1e-01   50      0.07457143  0.7465915  0.05385579
##   1e-04   1e-01  100      0.07457143  0.7465915  0.05385579
##   1e-04   1e-01  150      0.07457143  0.7465915  0.05385579
##   1e-01   0e+00   50      0.07871170  0.7225830  0.05702163
##   1e-01   0e+00  100      0.07871164  0.7225830  0.05702156
##   1e-01   0e+00  150      0.07871158  0.7225829  0.05702149
##   1e-01   1e-04   50      0.07872647  0.7221352  0.05712342
##   1e-01   1e-04  100      0.07872644  0.7221352  0.05712337
##   1e-01   1e-04  150      0.07872640  0.7221352  0.05712333
##   1e-01   1e-01   50      0.07439612  0.7473093  0.05330426
##   1e-01   1e-01  100      0.07439612  0.7473093  0.05330426
##   1e-01   1e-01  150      0.07439612  0.7473093  0.05330426
## 
## Tuning parameter 'eta' was held constant at a value of 0.3
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were nrounds = 50, lambda = 0.1, alpha
##  = 0.1 and eta = 0.3.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{models}\OperatorTok{$}\NormalTok{knn}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## k-Nearest Neighbors 
## 
## 199 samples
##   5 predictor
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 199, 199, 199, 199, 199, 199, ... 
## Resampling results across tuning parameters:
## 
##   k  RMSE        Rsquared   MAE       
##   5  0.08658990  0.6601111  0.06548773
##   7  0.08444153  0.6718092  0.06346668
##   9  0.08393995  0.6743269  0.06253178
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was k = 9.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{models}\OperatorTok{$}\NormalTok{Rborist}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Random Forest 
## 
## 199 samples
##   5 predictor
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 199, 199, 199, 199, 199, 199, ... 
## Resampling results across tuning parameters:
## 
##   predFixed  RMSE        Rsquared   MAE       
##   2          0.06905907  0.7774871  0.04954272
##   3          0.07031808  0.7701666  0.05065089
##   5          0.07271757  0.7561023  0.05272761
## 
## Tuning parameter 'minNode' was held constant at a value of 3
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were predFixed = 2 and minNode = 3.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{models}\OperatorTok{$}\NormalTok{glmnet}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## glmnet 
## 
## 199 samples
##   5 predictor
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 199, 199, 199, 199, 199, 199, ... 
## Resampling results across tuning parameters:
## 
##   alpha  lambda        RMSE        Rsquared   MAE       
##   0.10   0.0002568282  0.06495079  0.8067987  0.04742797
##   0.10   0.0025682822  0.06490501  0.8071440  0.04730252
##   0.10   0.0256828219  0.06613262  0.8042026  0.04734771
##   0.55   0.0002568282  0.06495173  0.8068342  0.04740708
##   0.55   0.0025682822  0.06497479  0.8069613  0.04731463
##   0.55   0.0256828219  0.06797812  0.8074955  0.04961532
##   1.00   0.0002568282  0.06496929  0.8067131  0.04742983
##   1.00   0.0025682822  0.06508041  0.8066258  0.04737834
##   1.00   0.0256828219  0.07188462  0.8010876  0.05383877
## 
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 0.1 and lambda = 0.002568282.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{models}\OperatorTok{$}\NormalTok{brnn}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Bayesian Regularized Neural Networks 
## 
## 199 samples
##   5 predictor
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 199, 199, 199, 199, 199, 199, ... 
## Resampling results across tuning parameters:
## 
##   neurons  RMSE        Rsquared   MAE       
##   1        0.06508188  0.8054340  0.04721833
##   2        0.06648537  0.7965571  0.04807210
##   3        0.06747989  0.7899181  0.04880175
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was neurons = 1.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{stopCluster}\NormalTok{(cl)}
\KeywordTok{registerDoSEQ}\NormalTok{()}
\KeywordTok{rm}\NormalTok{(cl) }\CommentTok{# removing cluster}
\KeywordTok{varImp}\NormalTok{(models}\OperatorTok{$}\NormalTok{Rborist) }\CommentTok{# variable importance for Rborist}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rborist variable importance
## 
##             Overall
## CGPA         100.00
## GRE.Score     95.78
## TOEFL.Score   44.33
## LOR           13.91
## Research       0.00
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{varImp}\NormalTok{(models}\OperatorTok{$}\NormalTok{glmnet)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## glmnet variable importance
## 
##              Overall
## CGPA        100.0000
## LOR          17.4840
## Research      7.2316
## TOEFL.Score   0.5354
## GRE.Score     0.0000
\end{verbatim}

\hypertarget{feature-reduction}{%
\subsection{Feature Reduction}\label{feature-reduction}}

The varImp procedure from the ensemble reveals different feature
importance based on the model. This step attempts to explore whether a
feature reduction is possible to achieve without compromising the
performance of the model using the random Forest function. A recursive
feature reduction method is implemented to check the feature reduction
opportunities.The recursive feature eliminated Research feature from the
modelo. Since the ensemble of models prefer to have different input
features, all features were left in the model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{sample.kind =} \StringTok{"Rounding"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in set.seed(1, sample.kind = "Rounding"): non-uniform 'Rounding' sampler
## used
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cl=}\KeywordTok{makePSOCKcluster}\NormalTok{(}\KeywordTok{detectCores}\NormalTok{()}\OperatorTok{-}\DecValTok{1}\NormalTok{)}
\KeywordTok{registerDoParallel}\NormalTok{(cl)}
 
\NormalTok{ctrl=}\KeywordTok{rfeControl}\NormalTok{(}\DataTypeTok{functions =}\NormalTok{ rfFuncs, }\CommentTok{#random forest function }
           \DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\CommentTok{# cross validation}
           \DataTypeTok{number =} \DecValTok{2}\NormalTok{, }\CommentTok{# times}
           \DataTypeTok{verbose =}\NormalTok{ F)}
\NormalTok{subsets=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{)}
\KeywordTok{registerDoSEQ}\NormalTok{() }\CommentTok{# avoid warning of parallel clusters not existing}
\NormalTok{lmProfile=}\KeywordTok{rfe}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(traindf[,}\OperatorTok{-}\DecValTok{6}\NormalTok{]), }
              \KeywordTok{as.matrix}\NormalTok{(traindf[,}\DecValTok{6}\NormalTok{]), }
              \DataTypeTok{sizes=}\NormalTok{subsets, }
              \DataTypeTok{rfeControl=}\NormalTok{ctrl) }\CommentTok{# recursive feature elimination}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in rfout$mse/(var(y) * (n - 1)/n): Recycling array of length 1 in vector-array arithmetic is deprecated.
##   Use c() or as.vector() instead.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lmProfile}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Recursive feature selection
## 
## Outer resampling method: Cross-Validated (2 fold) 
## 
## Resampling performance over subset size:
## 
##  Variables    RMSE Rsquared     MAE   RMSESD RsquaredSD    MAESD Selected
##          1 0.08702   0.6491 0.06394 0.024352    0.16673 0.019171         
##          2 0.07245   0.7511 0.05325 0.007942    0.04575 0.003054         
##          3 0.07289   0.7493 0.05217 0.008247    0.04638 0.002315         
##          4 0.06742   0.7857 0.04788 0.009911    0.05166 0.002970        *
##          5 0.06969   0.7769 0.05048 0.009760    0.04848 0.003595         
## 
## The top 4 variables (out of 4):
##    CGPA, GRE.Score, TOEFL.Score, LOR
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(lmProfile)}
\end{Highlighting}
\end{Shaded}

\includegraphics{USGradAdmission_files/figure-latex/feature_reduction-1.pdf}

The lmProfile chose 4 models. Nonetheless, owing to multiple models
involved in the ensemble, I have chosen to include all five features in
the model.

\hypertarget{cross-validation}{%
\subsection{Cross validation}\label{cross-validation}}

The following plot demonstrates the performance of the machine learning
models with the cross-validation dataset. The brnn and glmnet both
registered the lowest RMSE values in this step. The knn model registerd
the highest RMSE value. Glmnet which is a form of the general linear
model outperformed all other models in the cross-validation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ cl=}\KeywordTok{makePSOCKcluster}\NormalTok{(}\KeywordTok{detectCores}\NormalTok{()}\OperatorTok{-}\DecValTok{1}\NormalTok{)}
 \KeywordTok{registerDoParallel}\NormalTok{(cl)}
 \CommentTok{#retrieve resamples from cross validation}
\NormalTok{ resamples<-}\KeywordTok{resamples}\NormalTok{(models)}
 \KeywordTok{dotplot}\NormalTok{(resamples, }\DataTypeTok{metric=}\StringTok{"RMSE"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{USGradAdmission_files/figure-latex/cross_validation_results-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
 \KeywordTok{summary}\NormalTok{(resamples)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## summary.resamples(object = resamples)
## 
## Models: Rborist, knn, glmnet, xgbLinear, brnn, ridge 
## Number of resamples: 25 
## 
## MAE 
##                 Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
## Rborist   0.04087272 0.04650832 0.04837651 0.04954272 0.05306860 0.06023343
## knn       0.05062886 0.05933917 0.06375611 0.06253178 0.06639950 0.07113544
## glmnet    0.04017813 0.04434533 0.04664781 0.04730252 0.04891365 0.05898725
## xgbLinear 0.04162523 0.04886742 0.05361918 0.05330426 0.05739933 0.06370941
## brnn      0.03957659 0.04443392 0.04668988 0.04721833 0.04853141 0.05911275
## ridge     0.04039075 0.04496411 0.04685090 0.04749992 0.04864804 0.05971269
##           NA's
## Rborist      0
## knn          0
## glmnet       0
## xgbLinear    0
## brnn         0
## ridge        0
## 
## RMSE 
##                 Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
## Rborist   0.05888242 0.06372738 0.06814136 0.06905907 0.07374948 0.08176403
## knn       0.06539133 0.08009600 0.08325039 0.08393995 0.08835281 0.09735443
## glmnet    0.05327690 0.05940444 0.06664181 0.06490501 0.06906988 0.07601822
## xgbLinear 0.06361614 0.06704974 0.07401360 0.07439612 0.08120587 0.08926259
## brnn      0.05298541 0.05856899 0.06685617 0.06508188 0.06916823 0.07734920
## ridge     0.05347100 0.05981185 0.06631941 0.06496590 0.06934683 0.07660255
##           NA's
## Rborist      0
## knn          0
## glmnet       0
## xgbLinear    0
## brnn         0
## ridge        0
## 
## Rsquared 
##                Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
## Rborist   0.6565284 0.7323901 0.7859326 0.7774871 0.8201107 0.8508976    0
## knn       0.5287886 0.6358892 0.6652869 0.6743269 0.7100946 0.8093582    0
## glmnet    0.6955158 0.7847301 0.8143828 0.8071440 0.8376338 0.8644957    0
## xgbLinear 0.6395155 0.7255789 0.7519061 0.7473093 0.7941205 0.8330314    0
## brnn      0.6895725 0.7784176 0.8148005 0.8054340 0.8355610 0.8667472    0
## ridge     0.6937151 0.7860366 0.8144191 0.8066642 0.8386284 0.8631465    0
\end{verbatim}

\hypertarget{ensemble-models}{%
\subsection{Ensemble models}\label{ensemble-models}}

Ensembling is a technique where the model results are weighed according
to their performance. The caretEnsemble function performs this step and
we can predict the performance of the ensemble models with the
cross-validation dataset. The performance results are similar to those
found from the individual models.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cl=}\KeywordTok{makePSOCKcluster}\NormalTok{(}\KeywordTok{detectCores}\NormalTok{())}
\KeywordTok{registerDoParallel}\NormalTok{(cl)}
\CommentTok{# }
\NormalTok{ens=}\KeywordTok{caretEnsemble}\NormalTok{(models, }\DataTypeTok{metric=}\StringTok{"RMSE"}\NormalTok{, }
                  \DataTypeTok{trControl=}\NormalTok{my.con) }\CommentTok{# Run Ensemble model to gather the best result}
\KeywordTok{summary}\NormalTok{(ens)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## The following models were ensembled: Rborist, knn, glmnet, xgbLinear, brnn, ridge 
## They were weighted: 
## -0.0129 0.0473 -0.0222 1.7555 0.0816 0.3364 -1.1817
## The resulting RMSE is: 0.0652
## The fit for each individual model on the RMSE is: 
##     method       RMSE      RMSESD
##    Rborist 0.06905907 0.007154672
##        knn 0.08393995 0.008092117
##     glmnet 0.06490501 0.006347584
##  xgbLinear 0.07439612 0.007735828
##       brnn 0.06508188 0.006776304
##      ridge 0.06496590 0.006371914
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(ens)}
\end{Highlighting}
\end{Shaded}

\includegraphics{USGradAdmission_files/figure-latex/ensemble_output-1.pdf}

\hypertarget{validation}{%
\subsection{Validation}\label{validation}}

In this step, the model are evaluated for their performance against a
new and independent dataset. The models are used to predict the
admission probabilities of the students with the validation dataset
which was reserved from participating in the model developent process.
The actual values of the admission rates were compared to the predicted
values from various models. Both RMSE an the R2 values were evaluated.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{######### Results- Validation ############}

\CommentTok{#Prediction from Linear model }
\NormalTok{pred.lm.step=}\KeywordTok{predict}\NormalTok{(mod.lm.step, }\DataTypeTok{newdata =}\NormalTok{ valdf) }

\CommentTok{#Predicted from each ML model}

\NormalTok{ predicted.Rborist=}\KeywordTok{predict}\NormalTok{(models}\OperatorTok{$}\NormalTok{Rborist, }\DataTypeTok{newdata=}\NormalTok{valdf)}
\NormalTok{ predicted.knn=}\KeywordTok{predict}\NormalTok{(models}\OperatorTok{$}\NormalTok{knn, }\DataTypeTok{newdata=}\NormalTok{valdf)}
\NormalTok{ predicted.glmnet=}\KeywordTok{predict}\NormalTok{(models}\OperatorTok{$}\NormalTok{glmnet, }\DataTypeTok{newdata=}\NormalTok{valdf)}
\NormalTok{ predicted.xgbLinear=}\KeywordTok{predict}\NormalTok{(models}\OperatorTok{$}\NormalTok{xgbLinear, }\DataTypeTok{newdata=}\NormalTok{valdf)}
\NormalTok{ predicted.brnn=}\KeywordTok{predict}\NormalTok{(models}\OperatorTok{$}\NormalTok{brnn, }\DataTypeTok{newdata=}\NormalTok{valdf)}
\NormalTok{ predicted.ridge=}\KeywordTok{predict}\NormalTok{(models}\OperatorTok{$}\NormalTok{ridge, }\DataTypeTok{newdata=}\NormalTok{valdf)}
 
 
 
 
\CommentTok{#Prediction from Ensemble of the models}
\NormalTok{ predicted.ens=}\KeywordTok{predict}\NormalTok{(ens, }\DataTypeTok{newdata=}\NormalTok{valdf)}
 
  
 \CommentTok{# Construct the table to output th results.}
 \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{Rborist=}\KeywordTok{RMSE}\NormalTok{(predicted.Rborist, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit), }
            \DataTypeTok{Ensemble=}\KeywordTok{RMSE}\NormalTok{(predicted.ens, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit),}
            \DataTypeTok{Linear=}\KeywordTok{RMSE}\NormalTok{(pred.lm.step, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit))->rmses}
 
 \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{Rborist=}\KeywordTok{cor}\NormalTok{(predicted.Rborist, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit), }
            \DataTypeTok{Ensemble=}\KeywordTok{cor}\NormalTok{(predicted.ens, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit),}
            \DataTypeTok{Linear=}\KeywordTok{cor}\NormalTok{(pred.lm.step, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit))->r2s}
 
\NormalTok{ Results=}\KeywordTok{data_frame}\NormalTok{(}\DataTypeTok{Model=}\StringTok{"Linear-Stepwise-Selected"}\NormalTok{, }
                    \DataTypeTok{RMSE=}\KeywordTok{round}\NormalTok{(}\KeywordTok{RMSE}\NormalTok{(pred.lm.step, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit), }\DecValTok{3}\NormalTok{), }
                    \DataTypeTok{R2=}\KeywordTok{round}\NormalTok{(}\KeywordTok{cor}\NormalTok{(pred.lm.step, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit), }\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: `data_frame()` is deprecated as of tibble 1.1.0.
## Please use `tibble()` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Results[}\DecValTok{2}\NormalTok{,]=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{Model=}\StringTok{"knn"}\NormalTok{,}
                    \DataTypeTok{RMSE=}\KeywordTok{round}\NormalTok{(}\KeywordTok{RMSE}\NormalTok{(predicted.knn, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit), }\DecValTok{3}\NormalTok{), }
                    \DataTypeTok{R2=}\KeywordTok{round}\NormalTok{(}\KeywordTok{cor}\NormalTok{(predicted.knn, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit), }\DecValTok{3}\NormalTok{))}
\NormalTok{Results[}\DecValTok{3}\NormalTok{,]=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{Model=}\StringTok{"GLMnet"}\NormalTok{, }
                    \DataTypeTok{RMSE=}\KeywordTok{round}\NormalTok{(}\KeywordTok{RMSE}\NormalTok{(predicted.glmnet, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit), }\DecValTok{3}\NormalTok{), }
                    \DataTypeTok{R2=}\KeywordTok{round}\NormalTok{(}\KeywordTok{cor}\NormalTok{(predicted.glmnet, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit), }\DecValTok{3}\NormalTok{))}

\NormalTok{Results[}\DecValTok{4}\NormalTok{,]=}\KeywordTok{list}\NormalTok{( }\DataTypeTok{Model=}\StringTok{"XGBLinear"}\NormalTok{, }
                    \DataTypeTok{RMSE=}\KeywordTok{round}\NormalTok{(}\KeywordTok{RMSE}\NormalTok{(predicted.xgbLinear, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit), }\DecValTok{3}\NormalTok{), }
                    \DataTypeTok{R2=}\KeywordTok{round}\NormalTok{(}\KeywordTok{cor}\NormalTok{(predicted.xgbLinear, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit), }\DecValTok{3}\NormalTok{))}
\NormalTok{Results[}\DecValTok{5}\NormalTok{,]=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{Model=}\StringTok{"brnn"}\NormalTok{, }
                    \DataTypeTok{RMSE=}\KeywordTok{round}\NormalTok{(}\KeywordTok{RMSE}\NormalTok{(predicted.brnn, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit), }\DecValTok{3}\NormalTok{), }
                    \DataTypeTok{R2=}\KeywordTok{round}\NormalTok{(}\KeywordTok{cor}\NormalTok{(predicted.brnn, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit), }\DecValTok{3}\NormalTok{))}
\NormalTok{Results[}\DecValTok{6}\NormalTok{,]=}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{Model=}\StringTok{"Ridge Regression"}\NormalTok{, }
                    \DataTypeTok{RMSE=}\KeywordTok{round}\NormalTok{(}\KeywordTok{RMSE}\NormalTok{(predicted.ridge, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit), }\DecValTok{3}\NormalTok{), }
                    \DataTypeTok{R2=}\KeywordTok{round}\NormalTok{(}\KeywordTok{cor}\NormalTok{(predicted.ridge, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit), }\DecValTok{3}\NormalTok{))}

\NormalTok{Results[}\DecValTok{7}\NormalTok{,]=}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{Model=}\StringTok{"Ensemble"}\NormalTok{, }
                    \DataTypeTok{RMSE=}\KeywordTok{round}\NormalTok{(}\KeywordTok{RMSE}\NormalTok{(predicted.ens, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit), }\DecValTok{3}\NormalTok{), }
                    \DataTypeTok{R2=}\KeywordTok{round}\NormalTok{(}\KeywordTok{cor}\NormalTok{(predicted.ens, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit), }\DecValTok{3}\NormalTok{))}
\NormalTok{Results[}\DecValTok{8}\NormalTok{,]=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{Model=}\StringTok{"Rborist"}\NormalTok{, }
                    \DataTypeTok{RMSE=}\KeywordTok{round}\NormalTok{(}\KeywordTok{RMSE}\NormalTok{(predicted.Rborist, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit), }\DecValTok{3}\NormalTok{), }
                    \DataTypeTok{R2=}\KeywordTok{round}\NormalTok{(}\KeywordTok{cor}\NormalTok{(predicted.Rborist, valdf}\OperatorTok{$}\NormalTok{Chance.of.Admit), }\DecValTok{3}\NormalTok{))}
 


 
\KeywordTok{kable}\NormalTok{(Results, }\DataTypeTok{format=}\StringTok{"pandoc"}\NormalTok{, }
      \DataTypeTok{digits=}\DecValTok{3}\NormalTok{, }\DataTypeTok{caption =} \StringTok{"Performance of the different models: RMSE and R2 values"}\NormalTok{ ) }
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrr@{}}
\caption{Performance of the different models: RMSE and R2
values}\tabularnewline
\toprule
Model & RMSE & R2\tabularnewline
\midrule
\endfirsthead
\toprule
Model & RMSE & R2\tabularnewline
\midrule
\endhead
Linear-Stepwise-Selected & 0.067 & 0.882\tabularnewline
knn & 0.076 & 0.840\tabularnewline
GLMnet & 0.065 & 0.887\tabularnewline
XGBLinear & 0.081 & 0.825\tabularnewline
brnn & 0.067 & 0.879\tabularnewline
Ridge Regression & 0.066 & 0.885\tabularnewline
Ensemble & 0.066 & 0.884\tabularnewline
Rborist & 0.069 & 0.873\tabularnewline
\bottomrule
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Results}\OperatorTok{%>%}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\KeywordTok{reorder}\NormalTok{(Model, }\OperatorTok{-}\NormalTok{RMSE), RMSE, }\DataTypeTok{fill=}\NormalTok{Model))}\OperatorTok{+}\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat=}\StringTok{"identity"}\NormalTok{)}\OperatorTok{+}\KeywordTok{theme_bw}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle=}\DecValTok{90}\NormalTok{))}\OperatorTok{+}\KeywordTok{xlab}\NormalTok{(}\StringTok{"Model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{USGradAdmission_files/figure-latex/validation_results-1.pdf}

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

Comparison was made between a linear regression model and a ensemble
technique with machine learning algorithms for their abilities to
predict the admission probability of applicants to US graduate schools.
The applicant characteristics such as GRE.Score, TOEFL.Score, CGPA, LOR
and SOP were positively correlated with the admission probabilities.
GLMnet outperformed all of the other models, with a RMSE of 0.065 and a
R2 value of 0.887. This model was closely followed by Ridge regression
and Ensemble. It is clear that, in this case, invidividual models can
perform better than the ensemble technique.

\hypertarget{limitations}{%
\subsection{Limitations}\label{limitations}}

Owing to the limited sample sizes, this model has its' limitations.
Tuning of parameters for invidividual models can offer opportunity to
improve the performance of the models.





\newpage
\singlespacing 
\end{document}